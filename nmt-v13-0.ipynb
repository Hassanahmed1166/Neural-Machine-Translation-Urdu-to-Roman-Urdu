{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13126898,"sourceType":"datasetVersion","datasetId":8315725}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\ndata_path = '/kaggle/input/urdu-roman/urdu_roman.xlsx'\n\n# Read the Excel file\ndf = pd.read_excel(data_path)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:38.506730Z","iopub.execute_input":"2025-09-21T16:44:38.507024Z","iopub.status.idle":"2025-09-21T16:44:40.502568Z","shell.execute_reply.started":"2025-09-21T16:44:38.507003Z","shell.execute_reply":"2025-09-21T16:44:40.501936Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"#Create array of [Urdu, Roman Urdu] pairs\ndata_array = df.values.tolist()\n\n# Print first 5 elements to check\nprint(data_array[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:40.503995Z","iopub.execute_input":"2025-09-21T16:44:40.504729Z","iopub.status.idle":"2025-09-21T16:44:40.512909Z","shell.execute_reply.started":"2025-09-21T16:44:40.504704Z","shell.execute_reply":"2025-09-21T16:44:40.512098Z"}},"outputs":[{"name":"stdout","text":"[['یوں تو نہ تیرے جسم میں ہیں زینہار ہاتھ', 'yuun to na tere jism men hain zinhar haath'], ['دینے کے اے کریم مگر ہیں ہزار ہاتھ', 'dene ke ai karim magar hain hazar haath'], ['انگڑائیوں میں پھیلتے ہیں بار بار ہاتھ', 'angdaiyon men phailte hain baar baar haath'], ['شیشہ کی سمت بڑھتے ہیں بے اختیار ہاتھ', 'shisha ki samt badhte hain be-ikhtiyar haath'], ['ڈوبے ہیں ترک سعی سے افسوس تو یہ ہے', 'duube hain tark-e-sai se afsos to ye hai']]\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"max_urdu_words = 0\nmax_roman_urdu_words = 0\nmax_urdu_text = \"\"\nmax_roman_urdu_text = \"\"\n\nfor urdu, roman_urdu in data_array:\n    urdu_word_count = len(urdu.split())\n    roman_urdu_word_count = len(roman_urdu.split())\n    \n    if urdu_word_count > max_urdu_words:\n        max_urdu_words = urdu_word_count\n        max_urdu_text = urdu\n    \n    if roman_urdu_word_count > max_roman_urdu_words:\n        max_roman_urdu_words = roman_urdu_word_count\n        max_roman_urdu_text = roman_urdu\n\nprint(f\"Maximum Urdu word count: {max_urdu_words} (Text: {max_urdu_text})\")\nprint(f\"Maximum Roman Urdu word count: {max_roman_urdu_words} (Text: {max_roman_urdu_text})\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:40.513799Z","iopub.execute_input":"2025-09-21T16:44:40.514030Z","iopub.status.idle":"2025-09-21T16:44:40.552381Z","shell.execute_reply.started":"2025-09-21T16:44:40.514000Z","shell.execute_reply":"2025-09-21T16:44:40.551829Z"}},"outputs":[{"name":"stdout","text":"Maximum Urdu word count: 25 (Text: اب مجھ سے ہو تو ہو بھی کیا ہے ساتھ وہ تو وہ بھی کیا اک بے ہنر اک بے ثمر میں اور مری آوارگی)\nMaximum Roman Urdu word count: 23 (Text: ye dil hi tha jo sah gaya vo baat aisi kah gaya kahne ko phir kya rah gaya ashkon ka dariya bah gaya)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# import re\n# import json\n# from collections import Counter, defaultdict\n\n# # Flatten sentences\n# all_sentences = [sent for pair in data_array for sent in pair]\n\n# # -------------------------------\n# # Initialize vocab\n# # -------------------------------\n# vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n# vocab_size = 8000\n# counter = Counter()\n\n# # Step 2a: Split words into characters initially\n# def split_to_chars(word):\n#     return list(word)\n\n# for sentence in all_sentences:\n#     words = sentence.strip().split()\n#     for word in words:\n#         chars = split_to_chars(word)\n#         chars.append('</w>')  # End of word symbol\n#         counter.update([''.join(chars)])\n\n# # -------------------------------\n# # Build WordPiece merges\n# # -------------------------------\n# def get_stats(counter):\n#     pairs = defaultdict(int)\n#     for word, freq in counter.items():\n#         symbols = word.split()\n#         for i in range(len(symbols)-1):\n#             pairs[symbols[i], symbols[i+1]] += freq\n#     return pairs\n\n# def merge_vocab(pair, counter):\n#     bigram = ' '.join(pair)\n#     new_counter = {}\n#     for word in counter:\n#         w_out = word.replace(bigram, ''.join(pair))\n#         new_counter[w_out] = counter[word]\n#     return new_counter\n\n# # Start with each word as chars with spaces\n# token_counter = {}\n# for word in counter:\n#     token_counter[' '.join(list(word))] = counter[word]\n\n# while len(vocab) < vocab_size:\n#     pairs = get_stats(token_counter)\n#     if not pairs:\n#         break\n#     best = max(pairs, key=pairs.get)\n#     token_counter = merge_vocab(best, token_counter)\n#     token = ''.join(best)\n#     if token not in vocab:\n#         vocab[token] = len(vocab)\n\n# # -------------------------------\n# # Save tokenizer\n# # -------------------------------\n# with open(\"wordpiece_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n#     json.dump(vocab, f, ensure_ascii=False, indent=2)\n\n# print(\"Tokenizer saved. Vocab size:\", len(vocab))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:40.553663Z","iopub.execute_input":"2025-09-21T16:44:40.553868Z","iopub.status.idle":"2025-09-21T16:44:40.558029Z","shell.execute_reply.started":"2025-09-21T16:44:40.553851Z","shell.execute_reply":"2025-09-21T16:44:40.557275Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import json\n\nwith open(\"wordpiece_vocab.json\", \"r\", encoding=\"utf-8\") as f:\n    vocab = json.load(f)\n\ninv_vocab = {v: k for k, v in vocab.items()}\nunk_id = vocab[\"<unk>\"]\npad_id = vocab[\"<pad>\"]\nsos_id = vocab[\"<sos>\"]\neos_id = vocab[\"<eos>\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:40.559064Z","iopub.execute_input":"2025-09-21T16:44:40.559335Z","iopub.status.idle":"2025-09-21T16:44:40.575879Z","shell.execute_reply.started":"2025-09-21T16:44:40.559306Z","shell.execute_reply":"2025-09-21T16:44:40.575139Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def wordpiece_tokenize(word, vocab):\n    # greedy longest-match-first\n    chars = list(word)\n    chars.append('</w>')\n    tokens = []\n    i = 0\n    while i < len(chars):\n        j = len(chars)\n        while j > i:\n            piece = ''.join(chars[i:j])\n            if piece in vocab:\n                tokens.append(vocab[piece])\n                i = j\n                break\n            j -= 1\n        else:\n            tokens.append(unk_id)\n            i += 1\n    return tokens\n\ndef sentence_to_ids(sentence, vocab):\n    tokens = []\n    for word in sentence.strip().split():\n        tokens.extend(wordpiece_tokenize(word, vocab))\n    tokens.append(eos_id)\n    return tokens\n\n# Example\nurdu_ids = [sentence_to_ids(pair[0], vocab) for pair in data_array]\nroman_ids = [sentence_to_ids(pair[1], vocab) for pair in data_array]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:40.576571Z","iopub.execute_input":"2025-09-21T16:44:40.576798Z","iopub.status.idle":"2025-09-21T16:44:41.150761Z","shell.execute_reply.started":"2025-09-21T16:44:40.576782Z","shell.execute_reply":"2025-09-21T16:44:41.150106Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def pad_sequences(sequences, pad_id, max_len=None):\n    if max_len is None:\n        max_len = max(len(seq) for seq in sequences)\n    padded = []\n    for seq in sequences:\n        padded.append(seq + [pad_id]*(max_len - len(seq)))\n    return padded\n\nmax_len_input = max(len(seq) for seq in urdu_ids)\nmax_len_output = max(len(seq) for seq in roman_ids)\n\nencoder_input = pad_sequences(urdu_ids, pad_id, max_len_input)\ndecoder_output = pad_sequences(roman_ids, pad_id, max_len_output)\n# print(encoder_input)\n# print(decoder_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:41.151476Z","iopub.execute_input":"2025-09-21T16:44:41.151704Z","iopub.status.idle":"2025-09-21T16:44:41.191441Z","shell.execute_reply.started":"2025-09-21T16:44:41.151687Z","shell.execute_reply":"2025-09-21T16:44:41.190795Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import torch\n\nencoder_input = torch.tensor(encoder_input, dtype=torch.long)\ndecoder_output = torch.tensor(decoder_output, dtype=torch.long)\nprint(encoder_input)\nprint(decoder_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:41.192338Z","iopub.execute_input":"2025-09-21T16:44:41.192566Z","iopub.status.idle":"2025-09-21T16:44:41.368541Z","shell.execute_reply.started":"2025-09-21T16:44:41.192548Z","shell.execute_reply":"2025-09-21T16:44:41.367964Z"}},"outputs":[{"name":"stdout","text":"tensor([[ 388,   77,   74,  ...,    0,    0,    0],\n        [2457,   62,  264,  ...,    0,    0,    0],\n        [3588, 4297,  388,  ...,    0,    0,    0],\n        ...,\n        [ 125,  403, 1017,  ...,    0,    0,    0],\n        [2445, 1693, 4181,  ...,    0,    0,    0],\n        [ 186, 1466,    7,  ...,    0,    0,    0]])\ntensor([[ 253,   97,   41,  ...,    0,    0,    0],\n        [2458,   63,  265,  ...,    0,    0,    0],\n        [ 257,    1,   72,  ...,    0,    0,    0],\n        ...,\n        [ 117,  404,  613,  ...,    0,    0,    0],\n        [ 650,  238,  225,  ...,    0,    0,    0],\n        [ 187,  559,  140,  ...,    0,    0,    0]])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import torch.nn as nn\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n        \n        # Encoder: 2-layer bidirectional LSTM\n        self.encoder = nn.LSTM(\n            input_size=embed_dim,\n            hidden_size=hidden_dim,\n            num_layers=2,\n            batch_first=True,\n            bidirectional=True\n        )\n        \n        # Decoder: 4-layer LSTM\n        self.decoder = nn.LSTM(\n            input_size=embed_dim,\n            hidden_size=hidden_dim*2,  # because encoder is bidirectional\n            num_layers=4,\n            batch_first=True\n        )\n        \n        # Fully connected layer to vocab\n        self.fc = nn.Linear(hidden_dim*2, vocab_size)\n\n    def forward(self, src, tgt):\n        # Embed input sequences\n        src_embed = self.embedding(src)\n        tgt_embed = self.embedding(tgt)\n        \n        # Encoder\n        enc_out, (hidden, cell) = self.encoder(src_embed)\n        \n        # Concatenate bidirectional hidden states for decoder initial state\n        hidden = torch.cat([hidden[-2], hidden[-1]], dim=1).unsqueeze(0).repeat(4, 1, 1)\n        cell = torch.cat([cell[-2], cell[-1]], dim=1).unsqueeze(0).repeat(4, 1, 1)\n        \n        # Decoder\n        dec_out, _ = self.decoder(tgt_embed, (hidden, cell))\n        \n        # Output logits\n        logits = self.fc(dec_out)\n        return logits\n\nvocab_size = len(vocab)\nembed_dim = 128\nhidden_dim = 256\n\nmodel = Seq2Seq(vocab_size, embed_dim, hidden_dim, pad_id)\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:44:41.369210Z","iopub.execute_input":"2025-09-21T16:44:41.369412Z","iopub.status.idle":"2025-09-21T16:44:41.513406Z","shell.execute_reply.started":"2025-09-21T16:44:41.369396Z","shell.execute_reply":"2025-09-21T16:44:41.512757Z"}},"outputs":[{"name":"stdout","text":"Seq2Seq(\n  (embedding): Embedding(8000, 128, padding_idx=0)\n  (encoder): LSTM(128, 256, num_layers=2, batch_first=True, bidirectional=True)\n  (decoder): LSTM(128, 512, num_layers=4, batch_first=True)\n  (fc): Linear(in_features=512, out_features=8000, bias=True)\n)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import os\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Prepare data with special tokens\n\nurdu_sentences = [\"<sos> \" + row[0] + \" <eos>\" for row in data_array]\nroman_sentences = [\"<sos> \" + row[1] + \" <eos>\" for row in data_array]\n\n# Split dataset: 50% train, 25% val, 25% test\ntrain_temp, test = train_test_split(data_array, test_size=0.5, random_state=12)\ntrain, val = train_test_split(train_temp, test_size=0.5, random_state=12)\n\ntrain = np.array(train)\nval   = np.array(val)\ntest  = np.array(test)\n\n# Step 3: Roman Urdu tokenizer (WordPiece from scratch)\n\ndef build_wordpiece_vocab(sentences, vocab_size=8000, min_freq=2):\n    # Initialize vocab with special tokens\n    vocab = {\"<pad>\": 0, \"<unk>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n    idx = 4\n\n    # Count all character sequences\n    freq = {}\n    for sent in sentences:\n        for word in sent.split():\n            word = word + \"</w>\"  # End-of-word marker\n            for i in range(len(word)):\n                for j in range(i+1, len(word)+1):\n                    subword = word[i:j]\n                    freq[subword] = freq.get(subword, 0) + 1\n\n    # Keep frequent subwords\n    for subword, _ in sorted(freq.items(), key=lambda x: -x[1]):\n        if subword not in vocab and freq[subword] >= min_freq:\n            vocab[subword] = idx\n            idx += 1\n        if idx >= vocab_size:\n            break\n\n    return vocab\n\nroman_word2idx = build_wordpiece_vocab(roman_sentences, vocab_size=8000)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\n# Convert NumPy arrays to PyTorch tensors\ntrainX_t = torch.tensor(trainX, dtype=torch.long)\ntrainY_t = torch.tensor(trainY, dtype=torch.long)\nvalX_t = torch.tensor(valX, dtype=torch.long)\nvalY_t = torch.tensor(valY, dtype=torch.long)\ntestX_t = torch.tensor(testX, dtype=torch.long)\ntestY_t = torch.tensor(testY, dtype=torch.long)\n\n# Create TensorDatasets\ntrain_dataset = TensorDataset(trainX_t, trainY_t)\nval_dataset = TensorDataset(valX_t, valY_t)\ntest_dataset = TensorDataset(testX_t, testY_t)\n\n# Define DataLoaders\nbatch_size = 32  # you can adjust\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check shapes of one batch from each loader\nfor batchX, batchY in train_loader:\n    print(\"Train batch:\")\n    print(\"  X shape:\", batchX.shape)  # [batch_size, max_src_len]\n    print(\"  Y shape:\", batchY.shape)  # [batch_size, max_tgt_len]\n    break\n\nfor batchX, batchY in val_loader:\n    print(\"Validation batch:\")\n    print(\"  X shape:\", batchX.shape)\n    print(\"  Y shape:\", batchY.shape)\n    break\n\nfor batchX, batchY in test_loader:\n    print(\"Test batch:\")\n    print(\"  X shape:\", batchX.shape)\n    print(\"  Y shape:\", batchY.shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:45:11.924390Z","iopub.status.idle":"2025-09-21T16:45:11.924687Z","shell.execute_reply.started":"2025-09-21T16:45:11.924519Z","shell.execute_reply":"2025-09-21T16:45:11.924533Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport math\nfrom nltk.translate.bleu_score import corpus_bleu\nimport numpy as np\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Hyperparameters\nSRC_VOCAB = urdu_vocab_size\nTRG_VOCAB = roman_vocab_size\nEMB_SIZE = 256\nENC_HID = 256\nDEC_HID = 256\nENC_LAYERS = 2  \nDEC_LAYERS = 4  \nDROPOUT = 0.2\nBATCH_SIZE = 32\nLR = 1e-4\nEPOCHS = 5\nTEACHER_FORCING_RATIO = 0.9\nPAD_IDX = 0\n\n# Reverse mapping for tokenizer to convert predicted ids to tokens\nindex2word_trg = {i: w for w, i in roman_tokenizer.word_index.items()}\nindex2word_trg[0] = \"\"  # pad\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=PAD_IDX)\n        self.rnn = nn.LSTM(\n            emb_dim,\n            hid_dim,\n            num_layers=n_layers,\n            bidirectional=True,\n            batch_first=True,\n            dropout=dropout if n_layers > 1 else 0.0\n        )\n        self.dropout = nn.Dropout(dropout)\n        # If bidirectional and n_layers, decoder initial hidden should combine directions\n        self.fc_hidden = nn.Linear(hid_dim * 2, hid_dim)\n        self.fc_cell = nn.Linear(hid_dim * 2, hid_dim)\n\n    def forward(self, src, src_lengths=None):\n        # src: [batch, src_len]\n        embedded = self.dropout(self.embedding(src))\n        outputs, (hidden, cell) = self.rnn(embedded)\n        # outputs: [batch, src_len, hid*2]\n        # hidden/cell: [n_layers*2, batch, hid]\n        # We will combine the last layer forward and backward for initialization\n        # Take the last layer pair\n        # hidden[-2] is forward, hidden[-1] is backward for last layer\n        forward_h = hidden[-2,:,:]\n        backward_h = hidden[-1,:,:]\n        forward_c = cell[-2,:,:]\n        backward_c = cell[-1,:,:]\n        hid_init = torch.tanh(self.fc_hidden(torch.cat((forward_h, backward_h), dim=1)))\n        cell_init = torch.tanh(self.fc_cell(torch.cat((forward_c, backward_c), dim=1)))\n        # Expand to decoder layers dimension\n        # decoder expects n_layers, so tile the single vector across DEC_LAYERS\n        hid_init = hid_init.unsqueeze(0).repeat(DEC_LAYERS, 1, 1)  # [dec_layers, batch, hid]\n        cell_init = cell_init.unsqueeze(0).repeat(DEC_LAYERS, 1, 1)\n        return outputs, (hid_init, cell_init)\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        self.output_dim = output_dim\n        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=PAD_IDX)\n        self.rnn = nn.LSTM(\n            emb_dim,\n            hid_dim,\n            num_layers=n_layers,\n            batch_first=True,\n            dropout=dropout if n_layers > 1 else 0.0\n        )\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, input_step, hidden, cell):\n        # input_step: [batch] of token ids for current time step\n        input_step = input_step.unsqueeze(1)  # [batch, 1]\n        embedded = self.dropout(self.embedding(input_step))  # [batch, 1, emb]\n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        # output: [batch, 1, hid]\n        prediction = self.fc_out(output.squeeze(1))  # [batch, output_dim]\n        return prediction, hidden, cell\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg=None, teacher_forcing_ratio=0.5, max_len=None):\n        # src: [batch, src_len]\n        # trg: [batch, trg_len] or None for inference\n        batch_size = src.size(0)\n        if max_len is None:\n            max_len = trg.size(1) if trg is not None else 25\n        outputs = torch.zeros(batch_size, max_len, self.decoder.output_dim).to(self.device)\n        encoder_outputs, (hidden, cell) = self.encoder(src)\n        # first input token for decoder: usually start token.\n        # Keras Tokenizer does not have start token by default, so use PAD_IDX as placeholder.\n        # If you have a start token id, replace PAD_IDX below.\n        input_tok = torch.full((batch_size,), PAD_IDX, dtype=torch.long, device=self.device)\n        for t in range(0, max_len):\n            preds, hidden, cell = self.decoder(input_tok, hidden, cell)\n            outputs[:, t, :] = preds\n            if trg is not None and torch.rand(1).item() < teacher_forcing_ratio:\n                input_tok = trg[:, t]\n            else:\n                input_tok = preds.argmax(1)\n        return outputs\n\n# Build model\nenc = Encoder(SRC_VOCAB, EMB_SIZE, ENC_HID, ENC_LAYERS, DROPOUT)\ndec = Decoder(TRG_VOCAB, EMB_SIZE, DEC_HID, DEC_LAYERS, DROPOUT)\nmodel = Seq2Seq(enc, dec, device).to(device)\n\noptimizer = optim.Adam(model.parameters(), lr=LR)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n\n# Utility: translate predicted ids to token list\ndef ids_to_tokens(id_list):\n    tokens = []\n    for idx in id_list:\n        if idx == PAD_IDX:\n            break\n        token = index2word_trg.get(int(idx), \"\")\n        if token == \"\":\n            # unknown token or pad\n            continue\n        tokens.append(token.split())  # tokenizer is word-level, split to words\n    # flatten token lists if nested\n    flat = []\n    for t in tokens:\n        flat.extend(t)\n    return flat\n\n# Greedy decode for evaluation\ndef greedy_decode(model, src, max_len=25):\n    model.eval()\n    with torch.no_grad():\n        batch_size = src.size(0)\n        encoder_outputs, (hidden, cell) = model.encoder(src)\n        input_tok = torch.full((batch_size,), PAD_IDX, dtype=torch.long, device=device)\n        outputs = []\n        for t in range(max_len):\n            preds, hidden, cell = model.decoder(input_tok, hidden, cell)\n            input_tok = preds.argmax(1)\n            outputs.append(input_tok.unsqueeze(1))\n        outputs = torch.cat(outputs, dim=1)  # [batch, max_len]\n    return outputs\n\n# BLEU preparation: convert batch of token ids to lists of tokens\ndef batch_ids_to_texts(batch_ids):\n    texts = []\n    for seq in batch_ids:\n        words = []\n        for idx in seq.cpu().numpy():\n            if idx == PAD_IDX:\n                break\n            w = index2word_trg.get(int(idx), \"\")\n            if w != \"\":\n                words.append(w)\n        texts.append(words)\n    return texts\n\n# Training and evaluation loops\ndef run_epoch(loader, training=True):\n    epoch_loss = 0\n    all_refs = []\n    all_hyps = []\n    if training:\n        model.train()\n    else:\n        model.eval()\n    for src_batch, trg_batch in loader:\n        src_batch = src_batch.to(device)\n        trg_batch = trg_batch.to(device)\n        if training:\n            optimizer.zero_grad()\n        outputs = model(src_batch, trg_batch, teacher_forcing_ratio=TEACHER_FORCING_RATIO)\n        # outputs: [batch, trg_len, vocab]\n        output_dim = outputs.size(-1)\n        outputs_flat = outputs.view(-1, output_dim)\n        trg_flat = trg_batch.view(-1)\n        loss = criterion(outputs_flat, trg_flat)\n        if training:\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n        epoch_loss += loss.item() * src_batch.size(0)  # sum over batch\n\n        # collect for BLEU\n        with torch.no_grad():\n            preds_ids = outputs.argmax(-1)  # [batch, trg_len]\n            refs = batch_ids_to_texts(trg_batch)\n            hyps = batch_ids_to_texts(preds_ids)\n            # corpus_bleu expects list of list of references\n            for r, h in zip(refs, hyps):\n                if len(r) == 0:\n                    r = [\"\"]  # avoid empty\n                all_refs.append([r])\n                all_hyps.append(h)\n\n    total_tokens = len(loader.dataset)\n    avg_loss = epoch_loss / total_tokens\n    ppl = math.exp(avg_loss) if avg_loss < 700 else float(\"inf\")\n    # compute corpus BLEU\n    try:\n        bleu = corpus_bleu(all_refs, all_hyps)\n    except Exception:\n        bleu = 0.0\n    return avg_loss, ppl, bleu\n\n# Training loop\nfor epoch in range(1, EPOCHS + 1):\n    train_loss, train_ppl, train_bleu = run_epoch(train_loader, training=True)\n    val_loss, val_ppl, val_bleu = run_epoch(val_loader, training=False)\n    test_loss, test_ppl, test_bleu = run_epoch(test_loader, training=False)\n\n    print(f\"Epoch {epoch}\")\n    print(f\"Train  Loss {train_loss:.4f}  PPL {train_ppl:.4f}  BLEU {train_bleu:.4f}\")\n    print(f\"Val    Loss {val_loss:.4f}  PPL {val_ppl:.4f}  BLEU {val_bleu:.4f}\")\n    print(f\"Test   Loss {test_loss:.4f}  PPL {test_ppl:.4f}  BLEU {test_bleu:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T16:45:11.926027Z","iopub.status.idle":"2025-09-21T16:45:11.926239Z","shell.execute_reply.started":"2025-09-21T16:45:11.926143Z","shell.execute_reply":"2025-09-21T16:45:11.926152Z"}},"outputs":[],"execution_count":null}]}